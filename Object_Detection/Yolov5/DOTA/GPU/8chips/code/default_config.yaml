# Builtin Configurations(DO NOT CHANGE THESE CONFIGURATIONS unless you know exactly what you are doing)
enable_modelarts: False
# Url for modelarts
data_url: ""
train_url: ""
checkpoint_url: ""
outputs_url: ""
# Path for local
data_path: "/cache/data"
output_path: "/cache/train"
load_path: "/cache/checkpoint_path"
device_target: "GPU"
need_modelarts_dataset_unzip: True
modelarts_dataset_unzip_name: "coco"

# ==============================================================================
# Train options
data_dir: "/media/xx/新加卷/DATA/NWPU"
per_batch_size: 32

yolov5_version: "yolov5s"
pretrained_backbone: ""
resume_yolov5: ""
pretrained_checkpoint: ""
save_checkpoint_epochs: 1
output_dir: "/data02/tjy/OUTPUT/yolov5/test7_dota/train"
train_img_dir: "train2017"
train_json_file: "annotations/instances_train2017.json"
log_summary: False

lr_scheduler: "cosine_annealing"
lr: 0.0032
lr_epochs: "220,250"
lr_gamma: 0.1
eta_min: 0.0
T_max: 320         # please set 320 when run on 1p
max_epoch: 320       # please set 320 when run on 1p
warmup_epochs: 4  # please set 4 when run on 1p
weight_decay: 0.0005
momentum: 0.9
loss_scale: 1024.0
label_smooth: 0
label_smooth_factor: 0.1
log_interval: 100
ckpt_path: "outputs/"
is_distributed: 1
bind_cpu: True
device_num: 8
rank: 0
group_size: 1
need_profiler: 0
resize_rate: 10
filter_weight: False
use_modelarts: 1
obs_ckpt_path: ""

# Eval options
pretrained: "/data02/tjy/OUTPUT/yolov5/test7_dota/train/yolov5_320_189.ckpt"
log_path: "/data02/tjy/OUTPUT/yolov5/test7_dota/train/eval_log"
ann_val_file: ""
eval_nms_thresh: 0.6
ignore_threshold: 0.7
test_ignore_threshold: 0.001
multi_label: False
multi_label_thresh: 0.1

# infer options
infer_log: '/home/xx/Desktop/yolov5/output/infer/val_test'

# record
epochtime_path: '/data02/tjy/OUTPUT/yolov5/test7_dota/train/epochtime'
loss_path: '/data02/tjy/OUTPUT/yolov5/test7_dota/train/loss'

# Export options
device_id: 0
batch_size: 1
testing_shape: [640, 640]
ckpt_file: ""
file_name: "yolov5"
file_format: "MINDIR"
dataset_path: ""
ann_file: ""


# Other default config
hue: 0.015
saturation: 1.5
value: 0.4
jitter: 0.3

num_classes: 10
max_box: 640
checkpoint_filter_list: ['feature_map.back_block1.conv.weight', 'feature_map.back_block1.conv.bias',
                         'feature_map.back_block2.conv.weight', 'feature_map.back_block2.conv.bias',
                         'feature_map.back_block3.conv.weight', 'feature_map.back_block3.conv.bias']

# h->w
anchor_scales: [[12, 16],
                [19, 36],
                [40, 28],
                [36, 75],
                [76, 55],
                [72, 146],
                [142, 110],
                [192, 243],
                [459, 401]]

out_channel: 63  # 3 * (num_classes + 5)

input_shape: [[3, 32, 64, 128, 256, 512, 1],
              [3, 48, 96, 192, 384, 768, 2],
              [3, 64, 128, 256, 512, 1024, 3],
              [3, 80, 160, 320, 640, 1280, 4]]

# test_param
test_img_shape: [640, 640]

labels: ['plane', 'baseball-diamond', 'bridge', 'ground-track-field',
               'small-vehicle', 'large-vehicle', 'ship', 'tennis-court',
               'basketball-court', 'storage-tank', 'soccer-ball-field',
               'roundabout', 'harbor', 'swimming-pool', 'helicopter','container-crane']
#labels: [ 'airplane', 'ship', 'storage tank', 'baseball diamond', 'tennis court',
#'basketball court', 'ground track field', 'harbor', 'bridge', 'vehicle']

#coco_ids: [ 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]
coco_ids: [ 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16]

result_files: './result_Files'

---

# Help description for each configuration
# Train options
data_dir: "Train dataset directory."
per_batch_size: "Batch size for Training."
pretrained_backbone: "The ckpt file of CspDarkNet53."
resume_yolov5: "The ckpt file of YOLOv5, which used to fine tune."
pretrained_checkpoint: "The ckpt file of YOLOv5CspDarkNet53."
lr_scheduler: "Learning rate scheduler, options: exponential, cosine_annealing."
lr: "Learning rate."
lr_epochs: "Epoch of changing of lr changing, split with ','."
lr_gamma: "Decrease lr by a factor of exponential lr_scheduler."
eta_min: "Eta_min in cosine_annealing scheduler."
T_max: "T-max in cosine_annealing scheduler."
max_epoch: "Max epoch num to train the model."
warmup_epochs: "Warmup epochs."
weight_decay: "Weight decay factor."
momentum: "Momentum."
loss_scale: "Static loss scale."
label_smooth: "Whether to use label smooth in CE."
label_smooth_factor: "Smooth strength of original one-hot."
log_interval: "Logging interval steps."
ckpt_path: "Checkpoint save location."
ckpt_interval: "Save checkpoint interval."
is_save_on_master: "Save ckpt on master or all rank, 1 for master, 0 for all ranks."
is_distributed: "Distribute train or not, 1 for yes, 0 for no."
bind_cpu: "Whether bind cpu when distributed training."
device_num: "Device numbers per server"
rank: "Local rank of distributed."
group_size: "World size of device."
need_profiler: "Whether use profiler. 0 for no, 1 for yes."
resize_rate: "Resize rate for multi-scale training."
ann_file: "path to annotation"
each_multiscale: "Apply multi-scale for each scale"
labels: "the label of train data"
multi_label: "use multi label to nms"
multi_label_thresh: "multi label thresh"

# Eval options
pretrained: "model_path, local pretrained model to load"
log_path: "checkpoint save location"
ann_val_file: "path to annotation"

# Infer options
infer_log: "infer result path"

# Export options
device_id: "Device id for export"
batch_size: "batch size for export"
testing_shape: "shape for test"
ckpt_file: "Checkpoint file path for export"
file_name: "output file name for export"
file_format: "file format for export"
result_files: 'path to 310 infer result floder'
